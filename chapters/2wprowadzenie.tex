%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Rola metryk w in¿ynierii oprogramowania}
\label{rola-metryk}

\noindent Norma IEEE 1061-1998 \cite{749159} definiuje metrykê jako ``funkcjê odwzorowuj¹c¹ jednostkê oprogramowania w wartoœæ liczbow¹. Ta wyliczona wartoœæ jest interpretowalna jako stopieñ spe³nienia pewnej w³asnoœci jakoœci jednostki oprogramowania."

W in¿ynierii oprogramowania metryki s¹ wykorzystywane we wszystkich fazach procesu wytwarzania oprogramowania. Pozwalaj¹ na porównywanie ze sob¹ ró¿nych elementów lub ró¿nych projektów poniewa¿ s¹ danymi liczbowymi. W fazie projektowania mog¹ s³u¿yæ m.in. do szacowania nak³adu pracy potrzebnego do realizacji projektu. W fazie produkcji i testów do mierzenia jakoœci aplikacji, wydajnoœci pracy czy z³o¿onoœci programu.

Metryki mo¿na podzieliæ wed³ug ró¿nych kryeriów. Ze wzglêdu na typ artefaktu jaki opisuj¹ dzieli siê je na:
\begin{itemize}
	\item Metryki produktu (inaczej metryki kodu Ÿród³owego). S¹ bezpoœrednio wyliczane z kodu Ÿród³owego programu. Przyk³adem takich metryk s¹:
	\begin{itemize}
		\item Zestaw metryk CK \cite{chidamber1994metrics}, do którego nale¿¹:
		\begin{itemize}
			\item uœrednione metody na klasê (ang. \textit{Weighted Methods per Class}, WMC),
			\item g³êbokoœæ drzewa dziedziczenia (ang. \textit{Depth of Inheritance Tree}, DIT),
			\item liczba dzieci (ang. \textit{Number of Children}, NOC),
			\item zale¿noœæ miêdzy obiektami (ang. \textit{Coupling Between Objects}, CBO),
			\item odpowiedzialnoœæ danej klasy (ang. \textit{Response For a Class}, RFC),
			\item brak spójnoœci metod (ang. \textit{Lack of Cohesion of Methods}, LCOM).
		\end{itemize}
		\item OO --- metryki obiektowe, np.:
		\begin{itemize}
			\item liczba atrybutów (ang. \textit{Number of attributes}, NOA),
			\item liczba metod (ang. \textit{Number of methods}, NOM),
			\item liczba dziedziczonych metod (ang. \textit{Number of methods inherited}, NOMI).
		\end{itemize}
		\item LOC --- liczba linii kodu.
	\end{itemize}
	\item Metryki procesu (inaczej metryki zmian). Okreœlaj¹ zmiennoœæ atrybutu w czasie. Oblicza siê je dla zadanych przedzia³ów czasowych. Niezbêdna do ich obliczenia jest historia projektu, któr¹ mo¿na uzyskaæ dziêki systemom kontroli wersji (jak SVN czy Git). Przyk³ady metryk procesu:
		\begin{itemize}
			\item liczba modyfikacji (rewizji) pliku (ang. \textit{Number of Revisions}, NR),
			\item liczba autorów zmieniaj¹cych plik (ang. \textit{Number of Distinct Commiters}, NDC),
			\item liczba zmienionych linii kodu (ang. \textit{Number of Modi?ed Lines}, NML),
			\item wiek pliku (ang. \textit{Age}, AGE),
			\item liczba refaktoryzacji pliku (ang. \textit{(Number of Refactorings}, NREF),
			\item liczba dodanych (usuniêtych, zmienionych) metod,
			\item liczba dodanych (usuniêtych, zmienionych) atrybutów.
		\end{itemize}
\end{itemize}

Dodatkowo mo¿na podzieliæ metryki z uwagi na cel pomiaru \cite{gorski2000inzynieria}:
\begin{itemize}
	\item metryki z³o¿onoœci,
	\item metryki szacowania nak³adu,
	\item metryki funkcjonalnoœci.
\end{itemize}


Model predykcji defektów to narzêdzie, które na podstawie wartoœci metryk danego projektu dokonuje wskazania defektów znajduj¹cych siê w tym projekcie. Aby poprawnie zinterpretowaæ wskazania dostarczane przez model predykcji defektów zdefiniowaæ defekt. Norma 982.2 IEEE/ANSI \cite{26479} definiuje defekt jako anomaliê w produkcie, która mo¿e byæ:
\begin{itemize}
	\item zaniechaniami i niedoskona³oœciami znalezionymi podczas wczesnych faz cyklu ¿ycia oraz
	\item b³êdami zawartymi w oprogramowaniu wystarczaj¹co dojrza³ym do testowania lub dzia³ania.
\end{itemize}

Istniej¹ce badania wykaza³y, ¿e metryki procesu przewy¿szy³y metryki produktu w kontekœcie budowania modeli predykcji defektów \cite{giger2012method, kamei2010revisiting, mende2009revisiting, ferzund2009empirical}. Z tego powodu w dalszej czêœci pracy zrezygnowano z wykorzystania metryk produktu, bior¹c pod uwagê jedynie metryki procesu.



%Likewise, Kamei et al. (2010) revisited common findings in defect prediction when
%using effort-aware performance measurements. One finding that they confirmed is
%that process metrics (i.e., extracted from the version control system or the defect
%database) still perform better than product metrics (i.e., metrics of the software
%system itself).
%	Kamei Y, Matsumoto S, Monden A, Matsumoto K-i, Adams B, Hassan AE (2010) Revisit-
%	ing common bug prediction findings using effort aware models. In: Proceedings of the 26th
%	IEEE international conference on software maintenance (ICSM 2010). IEEE CS, Washington,
%	pp 1–10
%These findings corroborate those of Mende and Koschke (2009), and of Kamei
%et al. (2010), which found that product metrics were outperformed by process metrics
%when effort was taken into account.
%	Mende T, Koschke R (2009) Revisiting the evaluation of defect prediction models. In: Proceedings
%	of the 5th international conference on predictive models in software engineering (PROMISE
%	2009). ACM, New York, pp 1–10
%We have compared di?erent hunk metrics which represent product and process
%metrics. Process metrics have outperformed product metrics in prediction of
%bugs.
%	ferzunf2009empirical



%zastosowania metryk do ró¿nych celów
%
%krótko o rodzajach metryk, do czego mo¿na ich u¿yæ
%
%\begin{itemize}
%	\item metryki procesu daj¹ lepsze efekty
%	\item HCM?? \cite{hassan2009predicting}
%	\item metryki organizacji nie koreluj¹ z b³êdami \cite{hata2012bug}
%	\item wa¿noœæ historycznych metryk \cite{nagappan2005use, graves2000predicting, hassan2005top, kim2007predicting, hassan2009predicting, weyuker2008too, nagappan2008influence, mockus2010organizational, pinzger2008can, meneely2008predicting, wolf2009predicting, bird2011don, rahman2011ownership, moser2008comparative, kamei2010revisiting, czerwonka2011crane}
%\end{itemize}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Koszty zapewnienia jakoœci}

\noindent W in¿ynierii oprogramowania wystêpuje kilka ró¿nych definicji jakoœci. Na potrzeby niniejszej pracy przyjêto definicjê Kana \cite{kan2002metrics} ``brak defektów w produkcie". Zarz¹dzanie jakoœci¹ oprogramowania polega na podejmowaniu dzia³añ maj¹cych na celu zapewnienie jakoœci tworzonego oprogramowania poprzez szereg testów, które wspieraj¹ ca³y proces rozwoju oprogramowania.
\begin{itemize}
	\item Na etapie zbierania wymagañ --- weryfikacja czy okreœlone wymagania bêd¹ mo¿liwe do zweryfikowania (przetestowania).
	\item Na etapie projektowania --- zaplanowanie procesu testowego, wybór œrodowisk testowych.
	\item Na etapie kodowania --- definiowanie i realizacja scenariuszy i przypadków testowych oraz rejestracja defektów.
	\item Na etapie zamkniêcia projektu --- testy integracyjne, testy akceptacyjne, testy operacyjne.
\end{itemize}

\noindent Jak wykazano w \cite{arisholm2010systematic} koszty zapewnienia jakoœci s¹ prawie proporcjonalne do wielkoœci modu³u. Dlatego badacze bior¹ pod uwagê wysi³ek zwi¹zany z dzia³aniami maj¹cymi na celu zapewnienie jakoœci \cite{rahman2011bugcache, koru2008theory, menzies2010defect}. Zmniejszenie wysi³ku i kosztu zwi¹zanego z zapewnieniem jakoœci to obecnie jeden z g³ównych kierunków badañ \cite{hata2012bug}.

Podstawowym celem pracy dyplomowej jest stworzenie narzêdzi w postaci wtyczek do œrodowiska KNIME, s³u¿¹cych do gromadzenia metryk oprogramowania z systemów kontroli wersji oraz zgromadzenie jak najwiêkszej iloœci metryk w publicznym repozytorium. Nastêpnym krokiem jest stworzenie modelu (modeli) predykcji defektów oraz ich ewaluacja, bior¹c pod uwagê wysi³ek zwi¹zany z zapewnieniem jakoœci oprogramowania. Stworzenie narzêdzi pozwalaj¹cych na zautomatyzowane gromadzenie metryk z dostêpnych projektów (na przyk³ad Open Source) pozwoli rozszerzyæ publiczne zbiory danych. Dziêki temu bêdzie mo¿liwe wykorzystanie tych danych do tworzenia modeli predykcji defektów oprogramowania dziêki: wiêkszym zbiorom ucz¹cym; ewaluacji modeli na wiêkszych zbiorach danych.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Systemy kontroli wersji jako Ÿród³o danych o projektach}

\noindent Jak wspomniano wczeœniej w rozdziale \ref{rola-metryk} aby obliczyæ metryki procesu, konieczne jest uzyskanie historii projektu. Przegl¹d literatury pozwoli³ na wyodrêbnienie sposobów i narzêdzi, które pozwalaj¹ na porównywanie ró¿nych wersji kodu Ÿród³owego. Jak wykazano w \cite{kamei2010revisiting, posnett2011ecological, nguyen2010studying} predykcja na poziomie plików jest bardziej efektywna ni¿ na poziomie pakietów. Id¹c dalej w kierunku uszczegó³owienia wyników predykcji, mo¿na przypuszczaæ, ¿e predykcja na poziomie metod by³aby skuteczniejsza ni¿ na poziomie plików. Badanie \cite{hata2012bug} wykaza³o, ¿e pliki zawieraj¹ce b³êdy zawieraj¹ prawie lub ponad 10 metod, natomiast tylko kilka metod zawiera b³êdy (mediana 1--2). Jest to nie tylko odpowiedŸ na pytanie czy predykcja na poziomie metod jest skuteczniejsza, ale równie¿ wskazanie przyczyny takiego stanu rzeczy. Jednak¿e aby w pe³ni wykorzystaæ mo¿liwoœci ograniczenia kosztów jakoœci poprzez predykcjê na poziomie metod, potrzebne s¹ skuteczne modele, dostarczaj¹ce wiarygodnych wyników.

Ze wzglêdu na powy¿sze zale¿noœci, podjêto decyzjê o prowadzeniu dalszych prac w kierunku budowy modeli predykcji na poziomie metod. Poni¿ej wypisano techniki porównywania kodu Ÿród³owego na poziomie metod.

\begin{itemize}
	\item ChangeDistiller \cite{fluri2007change} --- polega na odwzorowaniu kodu Ÿród³owego Java w strukturze drzewiastej, jak¹ jest AST (ang. \textit{Abstract Syntax Tree}) a nastêpnie wyodrêbnieniu zmian pomiêdzy dwiema wersjami przy u¿yciu algorytmów porównywania drzew.
	\item Historage \cite{hata2011historage} --- wykorzystuje system kontroli wersji Git do przechowywania zidentyfikowanych zmian w kodzie na niskim poziomie.
	\item APFEL \cite{zimmermann2006fine} --- jest wtyczk¹ do œrodowiska Eclipse, która zbiera w bazie danych niskopoziomowe zmiany w kodzie. Dzia³a z systemem kontroli wersji SVS i Ÿród³ami Java.
	\item C-REX \cite{hassan2004c} --- wyodrêbnia fakty z historii kodu Ÿród³owego jêzyka C, a nastêpnie porównuje ze sob¹ kolejne wersje.
	\item Kenyon \cite{bevan2005facilitating}.
	\item Beagle \cite{godfrey2005using}.
\end{itemize}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Historia b³êdów w projektach}

\noindent Metryki, które stanowi¹ dane wejœciowe w modelach predykcji s¹ zmiennymi niezale¿nymi (ang. \textit{independent variables}). Pe³ny zestaw danych potrzebny do wytrenowania modelu obejmuje tak¿e zmienne zale¿ne (ang. \textit{dependent variables}). Zmienna niezale¿na reprezentuje wyjœcie (wynik), oraz mo¿e byæ u¿ywana do testowania modelu, ¿eby oceniæ jego skutecznoœæ. W predykcji defektów oprogramowania zmienn¹ zale¿n¹ jest liczba b³êdów lub zmienna okreœlaj¹ca czy wystêpuje b³¹d.

Aby uzyskaæ informacje o b³êdach w projekcie stosuje siê metody linkowania b³êdów. Linkowanie polega na odszukaniu powi¹zañ pomiêdzy zmian¹ zapisan¹ w repozytorium kodu, a b³êdem zg³oszonym w systemie œledzenia zmian (ang. \textit{Issue Tracking System}, ITS), takim jak JIRA, Bugzilla, IBM Rational ClearQuest czy innym.

Metoda u¿ywa w tej pracy opiera siê na metodzie SZZ \cite{sliwerski2005changes}. Jej zalet¹ jest porównywanie czasu naprawienia b³êdu zapisanego w ITS z czasem wys³ania poprawki do systemu kontroli wersji.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Przeznaczenie narzêdzi}

\noindent Narzêdzia stworzone w ramach pracy dyplomowej wchodz¹ w sk³ad platformy (ang. \textit{framework}) DePress\footnote{http://depress.io}. DePress (\textit{Defect Prediction in Software Systems}) jest rozszerzaln¹ platform¹ pozwalaj¹c¹ na budowanie przep³ywu pracy (ang. \textit{workflow}) w sposób graficzny, dziêki temu, ¿e jest oparty na projekcie KNIME. G³ównym celem DePress jest wspieranie analizy empirycznej oprogramowania. Pozwala na zbieranie, ³¹czenie i analizê danych z ró¿nych Ÿróde³, jak repozytoria oprogramowania czy metryki.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ograniczenia dotycz¹ce realizacji}

\noindent Poni¿ej wypisano ograniczenia dotycz¹ce realizacji badañ:
\begin{itemize}
	\item badanie tylko projektów Open Source,
	\item badanie tylko projektów napisanych w jêzyku Java,
	\item wykorzystanie narzêdzi platformy DePress, lub stworzenie nowych narzêdzi w ramach platformy,
	\item badania prowadzone w œrodowisku KNIME.
\end{itemize}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Mo¿liwoœci realizacji zadañ}
%
%\noindent Mo¿liwoœci realizacji zadañ - wstêpna dyskusja

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Metoda oceny}

\noindent Podstawow¹ ocen¹ efektywnoœci tworzonych modeli by³a ocena oparta na wysi³ku (ang. \textit{effort-based evaluation}). Dla ka¿dego modelu zosta³a stworzona krzywa efektywnoœci, przyk³ad takiej krzywej przedstawia rysunek \ref{example-chart}. Porównanie efektywnoœci modeli polega przede wszystkim na porównaniu procentowej iloœci b³êdów znalezionych w okreœlonej iloœci kodu. Przyjêto, ¿ê wartoœci¹ graniczn¹ kodu poddanego inspekcji bêdzie 20\%. Taka sama wartoœæ jest stosowana w innych badaniach. W przedstawionym przyk³adzie dokonuj¹c przegl¹du 20\% kodu, znajdzie siê w nim 30\% encji z defektami (w przypadku tych badañ s¹ to metody).

\begin{figure}[htbp]
	\caption{Wykres krzywej efektywnoœci}
	\centering
	\includegraphics[width=.75\textwidth]{charts/example}
	\label{example-chart}
\end{figure}

Oprócz okreœlenia wysi³ku, do oceny modeli przyjêto równie¿ inne miary. \todo{TODO dopisaæ jakie i jak siê je liczy, na pewno bêd¹: Accuracy, Cohen's kappa coefficient, AUC}

